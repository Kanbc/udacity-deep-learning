{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Convolutional Models\n",
    "## Design and train a Convolutional Neural Network\n",
    "\n",
    "*Note: The assignments in this course build on each other, so please finish them in order.*\n",
    "\n",
    "### Starter Code\n",
    "Open the iPython notebook for this assignment ([4_convolutions.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb)), and follow the instructions to implement and run each indicated step. Some steps have been implemented for you.\n",
    "\n",
    "### Evaluation\n",
    "This is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook).\n",
    "\n",
    "Improve the model by experimenting with its structure - how many layers, how they are connected, stride, pooling, etc. For more efficient training, try applying techniques such as dropout and learning rate decay. What does your final architecture look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels) (input เป็น cube มี $\\text{weight}\\cdot\\text{height}\\cdot\\text{dept}$)\n",
    "- labels as float 1-hot encodings. $[ 0 \\quad 1 \\quad 0 \\quad 0 \\quad \\cdots \\quad 0 \\quad 0 \\quad]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet: Two Convolutional Layers, One Fully Connected Layer and Softmax\n",
    "\n",
    "- Let's build a small network with two convolutional layers, followed by one fully connected layer.\n",
    "- Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes.\n",
    "\n",
    "### Part 1: Load Data & Build Computation Graph\n",
    "- **Weight Initialization**\n",
    "    - One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients.\n",
    "    - Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\".\n",
    "        - stddev=0.1\n",
    "        \n",
    "- **Calculating Output Size**\n",
    "    - $O = \\frac{W-K-2P}{S}+1$\n",
    "        - O is the output height/length\n",
    "        - W is the input height/length\n",
    "        - K is the filter size\n",
    "        - P is the padding\n",
    "            - \"same\" = -1\n",
    "            - \"valid\" = 0\n",
    "        - S is the stride\n",
    "        \n",
    "Manual Calculation of Image Output Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # - convolutional layars     \n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    # - fully connected\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    # - Output : Softmax Layer\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        # convolutional layars\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        # fully connected\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases # Output : Softmax Layer\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.737810\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50: 1.634895\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 100: 0.898253\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 150: 0.732456\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 74.2%\n",
      "Minibatch loss at step 200: 0.498377\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 250: 0.901544\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 300: 1.158538\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 77.9%\n",
      "Minibatch loss at step 350: 0.336097\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 400: 1.201921\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 450: 0.848476\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 500: 0.234797\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 550: 0.388615\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 600: 0.066860\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 650: 1.245653\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 700: 0.366870\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 750: 0.747333\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 800: 0.520797\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 850: 0.715814\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 900: 0.480849\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 950: 0.486373\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1000: 0.852859\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.9%\n",
      "Test accuracy: 87.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps  = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. \n",
    "- replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel(patch) size 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formats\n",
    "\n",
    "- tf.nn.conv2d(input, filter, strides, padding)\n",
    "- tf.nn.max_pool(value, ksize, strides, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how many layers?\n",
    "- how many conv layers?\n",
    "- K : the filter size \n",
    "- S : string size\n",
    "- P : Padding (\"same\" = -1, \"valid\" = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_size = 28x28, filter_size = 5, stride_side = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0 x 14.0\n",
      "7.0 x 7.0\n"
     ]
    }
   ],
   "source": [
    "# Output size calculation\n",
    "# for padding \"same\" which is -1\n",
    "output_1 = (28.00 - 5.00 - (2*-1))/2 + 1.00\n",
    "print(np.ceil(output_1),'x',np.ceil(output_1))\n",
    "output_2 = (output_1 - 5.00 - (2*-1))/2 + 1.00\n",
    "print(np.ceil(output_2),'x',np.ceil(output_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New function for Image Size: No Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# Create image size function based on input, filter size, padding and stride\n",
    "# 2 convolutions only\n",
    "def output_size_no_pool(input_size, filter_size, padding, conv_stride):\n",
    "    if padding == 'same':\n",
    "        padding = -1.00\n",
    "    elif padding == 'valid':\n",
    "        padding = 0.00\n",
    "    else:\n",
    "        return None\n",
    "    output_1 = float(((input_size - filter_size - 2*padding) / conv_stride) + 1.00)\n",
    "    output_2 = float(((output_1 - filter_size - 2*padding) / conv_stride) + 1.00)\n",
    "    return int(np.ceil(output_2))\n",
    "\n",
    "patch_size = 5\n",
    "final_image_size = output_size_no_pool(image_size, patch_size, padding='same', conv_stride=2)\n",
    "print(final_image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Load Data & Build Computation Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# patch_size = 5\n",
    "# depth = 16\n",
    "# num_hidden = 64\n",
    "\n",
    "batch_size = 16\n",
    "# Depth is the number of output channels \n",
    "# On the other hand, num_channels is the number of input channels set at 1 previously\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    '''Input data'''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    '''Variables'''\n",
    "    # Convolution 1 Layer\n",
    "    # Input channels: num_channels = 1\n",
    "    # Output channels: depth = 16\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    # Convolution 2 Layer\n",
    "    # Input channels: depth = 16\n",
    "    # Output channels: depth = 16\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    # Fully Connected Layer (Densely Connected Layer)\n",
    "    # Use neurons to allow processing of entire image\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([final_image_size * final_image_size * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    # Readout layer: Softmax Layer\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "    '''Model'''\n",
    "    def model(data):\n",
    "        # First Convolutional Layer with Pooling\n",
    "        conv_1 = tf.nn.conv2d(data, layer1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        hidden_1 = tf.nn.relu(conv_1 + layer1_biases)\n",
    "        pool_1 = tf.nn.max_pool(hidden_1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        # Second Convolutional Layer with Pooling\n",
    "        conv_2 = tf.nn.conv2d(pool_1, layer2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        hidden_2 = tf.nn.relu(conv_2 + layer2_biases)\n",
    "        pool_2 = tf.nn.max_pool(hidden_2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        # Full Connected Layer\n",
    "        shape = pool_2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool_2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        \n",
    "        # Readout Layer: Softmax Layer\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    '''Training computation'''\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "\n",
    "    '''Optimizer'''\n",
    "    # Learning rate of 0.05\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "    '''Predictions for the training, validation, and test data'''\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Run Computation & Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.075693\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 5000: 0.400405\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 10000: 0.795989\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 88.8%\n",
      "Minibatch loss at step 15000: 0.364743\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 20000: 0.461208\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 25000: 0.450123\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 5000 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "\n",
    "- image_size = 28\n",
    "- Convolutions\n",
    "    - conv_filter_size = 5\n",
    "    - conv_stride = 1\n",
    "- Average Pooling\n",
    "    - pool_filter_size = 2\n",
    "    - pool_stride = 2\n",
    "- padding='valid'\n",
    "- Prevent overfitting\n",
    "    - Learning rate decay\n",
    "    - Regularization\n",
    "    - Dropout\n",
    "- Layers\n",
    "    - Convolution\n",
    "    - Pooling\n",
    "    - Convolution\n",
    "    - Pooling\n",
    "    - Fully-connected\n",
    "    - Fully-connected\n",
    "    - Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
