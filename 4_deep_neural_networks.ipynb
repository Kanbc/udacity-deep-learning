{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/14.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks\n",
    "\n",
    "<img src=\"img/15.png\" width=\"800px\">\n",
    "\n",
    "\n",
    "In this lesson, you'll learn how to build multilayer neural networks with TensorFlow. Adding a hidden layer to a network allows it to model more complex functions. Also, using a non-linear activation function on the hidden layer lets it model non-linear functions.\n",
    "\n",
    "Next, you'll see how a ReLU hidden layer is implemented in TensorFlow.\n",
    "\n",
    "**Note**: Depicted above is a \"2-layer\" neural network:\n",
    "\n",
    "1. The first layer effectively consists of the set of weights and biases applied to X and passed through ReLUs. The output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer.\n",
    "2. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Rule in Ghaphical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/16.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "พอทำให้คำนวนแบบ Graph แล้วจะมีข้อดี 2 อย่างดังรูปข้างบน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/17.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทุกๆการทำ iteration จะคำนวณทั้ง forward prop และ back prop โดย backprop จะกินหน่วยความจำและประมวลผลมากกว่า forward prop 2 เท่า"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Stochastic Gradient Descent\n",
    "**Train a fully-connected network using Gradient Descent and Stochastic Gradient Descent**\n",
    "\n",
    "*Note: The assignments in this course build on each other, so please finish Assignment 1 before attempting this.*\n",
    "\n",
    "### Starter Code\n",
    "Open the iPython notebook for this assignment ([2_fullyconnected.ipynb](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb)), and follow the instructions to implement and/or run each indicated step. Some steps have been implemented for you.\n",
    "\n",
    "### Evaluation\n",
    "This is a self-evaluated assignment. As you go through the notebook, make sure you are able to solve each problem and answer any posed questions (save your responses as markdown in the notebook).\n",
    "\n",
    "Your new model should perform better than the one you developed for Assignment 1. Also, the time required to train using Stochastic Gradient Descent (SGD) should be considerably less than simple Gradient Descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ทำแยกไปในไฟล์ assignment 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Graphs and Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref : https://www.tensorflow.org/guide/graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensors_flowing.gif\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow has several advantages that TensorFlow leverages when executing your programs:\n",
    "\n",
    "- **Parallelism**. By using explicit edges to represent dependencies between operations, it is easy for the system to identify operations that can execute in parallel.\n",
    "\n",
    "- **Distributed execution**. By using explicit edges to represent the values that flow between operations, it is possible for TensorFlow to partition your program across multiple devices (CPUs, GPUs, and TPUs) attached to different machines. TensorFlow inserts the necessary communication and coordination between devices.\n",
    "\n",
    "- **Compilation**. TensorFlow's XLA compiler can use the information in your dataflow graph to generate faster code, for example, by fusing together adjacent operations.\n",
    "\n",
    "- **Portability**. The dataflow graph is a language-independent representation of the code in your model. You can build a dataflow graph in Python, store it in a SavedModel, and restore it in a C++ program for low-latency inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
